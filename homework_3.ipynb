{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework #3\n",
    "\n",
    "Goal: In this homework you will briefly tag a few documents for person references. You will then compare your tags to a set of tags produced by myself and possibly other people. For the purposes of the submission you need only submit the comparison information with the set I've tagged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    "Tag data. You'll want to sign up at https://dataturks.com/ . There you will want to create an account and setup a new project for tagging.\n",
    "\n",
    "Once you've created an account you'll want to create a new dataset. (+ button on the left hand side.)\n",
    "Select the Document Annotation option under Text annotations.\n",
    "    Provide a dataset name (you can use whatever)\n",
    "    for the list of entities just input Person\n",
    "    for the tagging instructions provide whatever you would like\n",
    "    Then hit submit\n",
    "    \n",
    "You will then hit the \"Upload raw data\" button. Here, go into the homework data directory and upload the zip file which has all the documents inside of it.\n",
    "\n",
    "You can then tag each document. Simply highlight the piece of text that you think is a person. When you've identified all of them for a single document hit \"move to done\". Use the guidelines contained in the next cell.\n",
    "\n",
    "When you finish tagging all 5 documents, go into the project you've tagged and hit the options button in the top right. You will then hit \"download\". You will want to have the \"complete items\" and \"json\" options selected. Hit download and you will then have a json file for all of your documents.\n",
    "\n",
    "(If this is confusing, please send me an email. I can try to come up with screen shots if necessary.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation guidelines:\n",
    "    These are from the ACE Guidelines for Person:\n",
    "    \n",
    "3.1 Persons (PER) \n",
    "    Each distinct person or set of people mentioned in a document refers to an entity of type Person. For example, people may be specified by name (“John Smith”), occupation (“the butcher”), family relation (“dad”), pronoun (“he”), etc., or by some combination of these. Dead people and human remains are to be recorded as entities of type Person. So are fictional human characters appearing in movies, TV, books, plays, etc. \n",
    "\n",
    "There are a number of words that are ambiguous as to their referent. For example, nouns, which normally refer to animals or non-humans, can be used to describe people. If it is clear to the annotator that the noun refers to a person in a given context, it should be marked as a Person entity. \n",
    "\n",
    "Examples: \n",
    "He is [a real turkey]\n",
    "[The political cat of the year]\n",
    "She’s known as [the brain of the family] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "\n",
    "Look through the below code.\n",
    "\n",
    "You will see that the compare annotations method needs to be implemented. It should report back the number of matches and non-matching annotations\n",
    "\n",
    "Implement the appropriate code to calculate these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# method to read annotation file\n",
    "def annotation_processor(annotation_file):\n",
    "    annotation_array = []\n",
    "\n",
    "    # here we need to be careful and process each line of the annotation file separately\n",
    "    read_annotation = open(annotation_file)\n",
    "    for line in read_annotation:\n",
    "        data = json.loads(line)\n",
    "        annotation_array.append(data)\n",
    "\n",
    "    # here we return an array of the individual annotations\n",
    "    return annotation_array\n",
    "    \n",
    "# calling the annotation processor function\n",
    "annotation_processor('/homework_3/annotated_data/annotated.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will create two objects to store the reference annotations and your own annotations\n",
    "reference_annotations = annotation_processor('/homework_3/annotated_data/annotated.json')\n",
    "\n",
    "# here I am just putting the same file in... if I do this I would expect a perfect match\n",
    "my_annotations = annotation_processor('/homework_3/annotated_data/annotated.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will primarily focus on implementing this compare_annotations method\n",
    "def compare_annotations(reference_annotations, my_annotations):\n",
    "    num_matches = 0\n",
    "    num_non_matches = 0\n",
    "    # you will need to implement this method\n",
    "    # The annotations you will get in the parameters are arrays of objects like this:\n",
    "    # {'label': ['Person'],\n",
    "    # 'points': [{'start': 85,\n",
    "    #  'end': 116,\n",
    "    #  'text': 'A group of students and teachers'}]}\n",
    "    # You will need to compare the annoatations between the reference and your own\n",
    "    # You will look to see if you have annotations that match when the start and end values are the same,\n",
    "    # that is the character offset is correct\n",
    "    \n",
    "    print(\"success\")\n",
    "    \n",
    "    return num_matches, num_non_matches\n",
    "    \n",
    "\n",
    "def compare_annotation_files(reference_annotation_array, my_annotation_array):\n",
    "    num_annotations_in_reference = 0\n",
    "    num_annotations_in_mine = 0\n",
    "    \n",
    "    # we want this method to calculate these two numbers\n",
    "    # num_matches should simply count all the cases where we \n",
    "    num_matches = 0\n",
    "    # num_non_matches should simply count those cases where an annotation only occurs in one file but not both\n",
    "    num_non_matches = 0\n",
    "    \n",
    "    # OPTIONAL if you want to be more precise you can look for annotations where this a partial overalp\n",
    "    num_partial_match = 0\n",
    "    \n",
    "    for annotation in reference_annotation_array:\n",
    "        for my_annotation in my_annotation_array:\n",
    "            # here we need to do some things to ensure that the documents we are comparing are identical\n",
    "            if (annotation[\"content\"] == my_annotation[\"content\"]):\n",
    "                temp_num_matches, temp_num_non_matches = compare_annotations(annotation[\"annotation\"], my_annotation[\"annotation\"])\n",
    "        \n",
    "        # implement the sum of the temp_num_matches to the num_matches\n",
    "        \n",
    "    return num_matches, num_non_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(compare_annotation_files(reference_annotations, my_annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:\n",
    "\n",
    "With the numbers now available, try to determine the Cohen's Kappa for this dataset. (You can calculate this by hand if you prefer). Assume that the Probability of random agreement is 0.3.\n",
    "\n",
    "Note, with this the proportionate agreement would be the number of matches divided by the number of matches + the number of non-matches.\n",
    "\n",
    "The rest of the formula should be as discussed in class.\n",
    "\n",
    "Report the numbers you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:\n",
    "\n",
    "You will want to write a short report of 1 or 2 paragraphs. In it you should describe:\n",
    "    What kind of differences there are between your annotations and the ones I have provided.\n",
    "        Look through the different annotations and suggest where I might have been mistaken in identifying people\n",
    "    Explain if you think tagging persons are difficult. Do you think the guidelines should be improved?\n",
    "    A brief explanation of the Cohen's Kappa you calculated. Do you think it might be high or low? Does it report anything useful?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
